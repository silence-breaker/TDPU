"""åˆ†æ BitNet b1.58 2B-4T æ¨¡å‹çš„ safetensors æ–‡ä»¶ç»“æ„ã€‚

è¾“å‡ºå†…å®¹:
1. æ‰€æœ‰å¼ é‡çš„åç§°ã€å½¢çŠ¶ã€æ•°æ®ç±»å‹
2. æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
3. å„ç±»å‚æ•°çš„å†…å­˜å ç”¨
4. æƒé‡å€¼åˆ†å¸ƒåˆ†æ (éªŒè¯ 1.58-bit ä¸‰å€¼ç‰¹æ€§)
"""

import json
import struct
from collections import defaultdict
from pathlib import Path

MODEL_DIR = Path(__file__).parent
SAFETENSORS_PATH = MODEL_DIR / "model.safetensors"
CONFIG_PATH = MODEL_DIR / "config.json"

# safetensors æ•°æ®ç±»å‹ -> (åç§°, å­—èŠ‚æ•°)
DTYPE_INFO = {
    "F64": ("float64", 8),
    "F32": ("float32", 4),
    "F16": ("float16", 2),
    "BF16": ("bfloat16", 2),
    "I64": ("int64", 8),
    "I32": ("int32", 4),
    "I16": ("int16", 2),
    "I8": ("int8", 1),
    "U8": ("uint8", 1),
    "BOOL": ("bool", 1),
}


def parse_safetensors_header(filepath: Path) -> dict:
    """è§£æ safetensors æ–‡ä»¶å¤´ï¼Œè¿”å›å…ƒæ•°æ®å­—å…¸ã€‚"""
    with open(filepath, "rb") as f:
        # å‰ 8 å­—èŠ‚æ˜¯ header é•¿åº¦ (little-endian uint64)
        header_len = struct.unpack("<Q", f.read(8))[0]
        header_bytes = f.read(header_len)
    metadata = json.loads(header_bytes)
    # ç§»é™¤ __metadata__ é”® (å¦‚æœå­˜åœ¨)
    metadata.pop("__metadata__", None)
    return metadata


def analyze_tensors(metadata: dict) -> list[dict]:
    """å°†å…ƒæ•°æ®è§£æä¸ºç»“æ„åŒ–çš„å¼ é‡ä¿¡æ¯åˆ—è¡¨ã€‚"""
    tensors = []
    for name, info in sorted(metadata.items()):
        dtype_str = info["dtype"]
        shape = info["shape"]
        offsets = info["data_offsets"]  # [start, end]
        num_elements = 1
        for dim in shape:
            num_elements *= dim
        _, bytes_per_elem = DTYPE_INFO.get(dtype_str, (dtype_str, 0))
        size_bytes = num_elements * bytes_per_elem
        tensors.append({
            "name": name,
            "shape": shape,
            "dtype": dtype_str,
            "num_elements": num_elements,
            "size_bytes": size_bytes,
            "offset_start": offsets[0],
            "offset_end": offsets[1],
        })
    return tensors


def classify_tensor(name: str) -> str:
    """æ ¹æ®å¼ é‡åç§°åˆ†ç±»ã€‚"""
    if "embed_tokens" in name:
        return "Embedding"
    if "lm_head" in name:
        return "LM Head"
    if "norm" in name and "layers" not in name:
        return "Final Norm"
    if "self_attn" in name:
        if "q_proj" in name:
            return "Attention Q"
        if "k_proj" in name:
            return "Attention K"
        if "v_proj" in name:
            return "Attention V"
        if "o_proj" in name:
            return "Attention O"
        return "Attention Other"
    if "mlp" in name:
        if "gate_proj" in name:
            return "MLP Gate"
        if "up_proj" in name:
            return "MLP Up"
        if "down_proj" in name:
            return "MLP Down"
        return "MLP Other"
    if "input_layernorm" in name or "post_attention_layernorm" in name:
        return "Layer Norm"
    return "Other"


def extract_layer_idx(name: str) -> int | None:
    """ä»å¼ é‡åç§°ä¸­æå–å±‚å·ã€‚"""
    parts = name.split(".")
    for i, part in enumerate(parts):
        if part == "layers" and i + 1 < len(parts):
            try:
                return int(parts[i + 1])
            except ValueError:
                pass
    return None


def format_size(size_bytes: int) -> str:
    """æ ¼å¼åŒ–å­—èŠ‚æ•°ä¸ºå¯è¯»å­—ç¬¦ä¸²ã€‚"""
    if size_bytes >= 1024 ** 3:
        return f"{size_bytes / 1024**3:.2f} GB"
    if size_bytes >= 1024 ** 2:
        return f"{size_bytes / 1024**2:.2f} MB"
    if size_bytes >= 1024:
        return f"{size_bytes / 1024:.2f} KB"
    return f"{size_bytes} B"


def main():
    # åŠ è½½ config
    with open(CONFIG_PATH) as f:
        config = json.load(f)

    print("=" * 80)
    print("BitNet b1.58 2B-4T æ¨¡å‹ç»“æ„åˆ†æ")
    print("=" * 80)

    print(f"\nğŸ“‹ æ¨¡å‹é…ç½® (config.json):")
    print(f"  æ¶æ„:           {config['architectures'][0]}")
    print(f"  éšè—å±‚ç»´åº¦:     {config['hidden_size']}")
    print(f"  ä¸­é—´å±‚ç»´åº¦:     {config['intermediate_size']}")
    print(f"  å±‚æ•°:           {config['num_hidden_layers']}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°:     {config['num_attention_heads']}")
    print(f"  KV å¤´æ•°:        {config['num_key_value_heads']}")
    print(f"  è¯è¡¨å¤§å°:       {config['vocab_size']}")
    print(f"  æœ€å¤§ä½ç½®ç¼–ç :   {config['max_position_embeddings']}")
    print(f"  æ¿€æ´»å‡½æ•°:       {config['hidden_act']}")
    print(f"  é‡åŒ–æ–¹å¼:       {config['quantization_config']['quant_method']}")
    print(f"  æ•°æ®ç±»å‹:       {config['torch_dtype']}")
    print(f"  tie_word_embed: {config['tie_word_embeddings']}")

    # è§£æ safetensors
    print(f"\nğŸ“¦ æ–‡ä»¶: {SAFETENSORS_PATH.name}")
    print(f"  å¤§å°: {format_size(SAFETENSORS_PATH.stat().st_size)}")

    metadata = parse_safetensors_header(SAFETENSORS_PATH)
    tensors = analyze_tensors(metadata)

    # â”€â”€ 1. æ‰€æœ‰å¼ é‡åˆ—è¡¨ â”€â”€
    print(f"\n{'=' * 80}")
    print(f"1. å¼ é‡åˆ—è¡¨ (å…± {len(tensors)} ä¸ª)")
    print(f"{'=' * 80}")
    print(f"{'åç§°':<65} {'å½¢çŠ¶':<25} {'ç±»å‹':<8} {'å¤§å°':>10}")
    print("-" * 110)
    for t in tensors:
        shape_str = str(t["shape"])
        print(f"{t['name']:<65} {shape_str:<25} {t['dtype']:<8} {format_size(t['size_bytes']):>10}")

    # â”€â”€ 2. æŒ‰ç±»åˆ«ç»Ÿè®¡ â”€â”€
    print(f"\n{'=' * 80}")
    print("2. æŒ‰ç±»åˆ«ç»Ÿè®¡")
    print(f"{'=' * 80}")
    category_stats = defaultdict(lambda: {"count": 0, "elements": 0, "bytes": 0})
    for t in tensors:
        cat = classify_tensor(t["name"])
        category_stats[cat]["count"] += 1
        category_stats[cat]["elements"] += t["num_elements"]
        category_stats[cat]["bytes"] += t["size_bytes"]

    total_bytes = sum(s["bytes"] for s in category_stats.values())
    total_elements = sum(s["elements"] for s in category_stats.values())

    print(f"{'ç±»åˆ«':<20} {'å¼ é‡æ•°':>6} {'å‚æ•°é‡':>15} {'å­˜å‚¨å¤§å°':>12} {'å æ¯”':>8}")
    print("-" * 65)
    for cat in sorted(category_stats, key=lambda c: category_stats[c]["bytes"], reverse=True):
        s = category_stats[cat]
        pct = s["bytes"] / total_bytes * 100
        print(f"{cat:<20} {s['count']:>6} {s['elements']:>15,} {format_size(s['bytes']):>12} {pct:>7.2f}%")
    print("-" * 65)
    print(f"{'åˆè®¡':<20} {len(tensors):>6} {total_elements:>15,} {format_size(total_bytes):>12} {'100.00%':>8}")

    # â”€â”€ 3. æŒ‰å±‚åˆ†ç»„ (Layer 0 è¯¦ç»†å±•ç¤º) â”€â”€
    print(f"\n{'=' * 80}")
    print("3. Layer 0 è¯¦ç»†ç»“æ„ (å…¶ä½™å±‚ç»“æ„ç›¸åŒ)")
    print(f"{'=' * 80}")
    for t in tensors:
        if extract_layer_idx(t["name"]) == 0:
            short_name = t["name"].split("model.layers.0.")[-1]
            shape_str = str(t["shape"])
            print(f"  {short_name:<50} {shape_str:<25} {t['dtype']:<8} {format_size(t['size_bytes']):>10}")

    # â”€â”€ 4. FPGA åŠ é€Ÿç›¸å…³åˆ†æ â”€â”€
    print(f"\n{'=' * 80}")
    print("4. FPGA åŠ é€Ÿç›¸å…³åˆ†æ (BitLinear å±‚)")
    print(f"{'=' * 80}")

    hidden = config["hidden_size"]
    inter = config["intermediate_size"]
    n_heads = config["num_attention_heads"]
    n_kv_heads = config["num_key_value_heads"]
    head_dim = hidden // n_heads
    n_layers = config["num_hidden_layers"]

    linear_layers = [
        ("q_proj", hidden, hidden),
        ("k_proj", hidden, n_kv_heads * head_dim),
        ("v_proj", hidden, n_kv_heads * head_dim),
        ("o_proj", hidden, hidden),
        ("gate_proj", hidden, inter),
        ("up_proj", hidden, inter),
        ("down_proj", inter, hidden),
    ]

    print(f"\n  æ¯å±‚ Transformer Block çš„ Linear å±‚:")
    print(f"  {'åç§°':<15} {'è¾“å…¥ç»´åº¦ (K)':>12} {'è¾“å‡ºç»´åº¦ (M)':>12} {'å‚æ•°é‡':>12} {'1.58-bit å‹ç¼©':>14}")
    print("  " + "-" * 68)
    layer_total_params = 0
    layer_total_compressed = 0
    for name, k, m in linear_layers:
        params = k * m
        # 1.58-bit: æ¯ä¸ªæƒé‡ 2 bit, å‹ç¼©åå¤§å°
        compressed = params * 2 // 8
        layer_total_params += params
        layer_total_compressed += compressed
        print(f"  {name:<15} {k:>12,} {m:>12,} {params:>12,} {format_size(compressed):>14}")
    print("  " + "-" * 68)
    print(f"  {'å•å±‚åˆè®¡':<15} {'':>12} {'':>12} {layer_total_params:>12,} {format_size(layer_total_compressed):>14}")
    print(f"  {'å…¨éƒ¨ ' + str(n_layers) + ' å±‚':<15} {'':>12} {'':>12} {layer_total_params * n_layers:>12,} {format_size(layer_total_compressed * n_layers):>14}")

    # â”€â”€ 5. æ•°æ®ç±»å‹åˆ†å¸ƒ â”€â”€
    print(f"\n{'=' * 80}")
    print("5. æ•°æ®ç±»å‹åˆ†å¸ƒ")
    print(f"{'=' * 80}")
    dtype_stats = defaultdict(lambda: {"count": 0, "bytes": 0})
    for t in tensors:
        dtype_stats[t["dtype"]]["count"] += 1
        dtype_stats[t["dtype"]]["bytes"] += t["size_bytes"]
    for dtype, s in sorted(dtype_stats.items(), key=lambda x: x[1]["bytes"], reverse=True):
        friendly, bpe = DTYPE_INFO.get(dtype, (dtype, 0))
        print(f"  {friendly:<10} ({bpe}B/elem): {s['count']:>4} å¼ é‡, {format_size(s['bytes']):>12}")

    # â”€â”€ 6. æƒé‡å€¼é‡‡æ ·åˆ†æ â”€â”€
    print(f"\n{'=' * 80}")
    print("6. æƒé‡å€¼é‡‡æ ·åˆ†æ (éªŒè¯ä¸‰å€¼ç‰¹æ€§)")
    print(f"{'=' * 80}")
    try:
        analyze_weight_values(tensors)
    except Exception as e:
        print(f"  è·³è¿‡ (éœ€è¦å®‰è£… safetensors åº“): {e}")
        print(f"  å®‰è£…: pip install safetensors")

    print()


def unpack_ternary_u8(packed: "numpy.ndarray") -> "numpy.ndarray":
    """å°† U8 æ‰“åŒ…çš„ 2-bit ä¸‰å€¼æƒé‡è§£åŒ…ä¸º {-1, 0, +1}ã€‚

    æ¯ä¸ª uint8 åŒ…å« 4 ä¸ª 2-bit æƒé‡ (LSB first):
      2'b00 -> -1, 2'b01 -> 0, 2'b10 -> +1, 2'b11 -> 0 (padding)
    """
    import numpy as np
    flat = packed.flatten().astype(np.uint8)
    # æå– 4 ä¸ª 2-bit å­—æ®µ
    w0 = flat & 0x03
    w1 = (flat >> 2) & 0x03
    w2 = (flat >> 4) & 0x03
    w3 = (flat >> 6) & 0x03
    # äº¤é”™åˆå¹¶: [w0_0, w1_0, w2_0, w3_0, w0_1, ...]
    codes = np.stack([w0, w1, w2, w3], axis=-1).reshape(-1)
    # æ˜ å°„: 0b00->-1, 0b01->0, 0b10->+1, 0b11->0
    mapping = np.array([-1, 0, 1, 0], dtype=np.int8)
    return mapping[codes]


def analyze_weight_values(tensors: list[dict]):
    """é‡‡æ ·åˆ†ææƒé‡å€¼ï¼ŒéªŒè¯æ˜¯å¦ä¸º {-1, 0, +1} ä¸‰å€¼ (U8 æ‰“åŒ…æ ¼å¼)ã€‚"""
    try:
        from safetensors import safe_open
        import numpy as np
    except ImportError:
        print("  éœ€è¦ safetensors + numpy åº“: pip install safetensors numpy")
        return

    # åªé€‰å– U8 ç±»å‹çš„ weight å¼ é‡ (æ‰“åŒ…çš„ä¸‰å€¼æƒé‡)
    sample_tensors = [
        t for t in tensors
        if t["dtype"] == "U8"
        and ("layers.0." in t["name"] or "layers.15." in t["name"])
        and any(p in t["name"] for p in ["q_proj.weight", "gate_proj.weight", "down_proj.weight"])
        and "scale" not in t["name"]
    ]

    if not sample_tensors:
        sample_tensors = [t for t in tensors if t["dtype"] == "U8" and "scale" not in t["name"]][:3]

    print(f"\n  è¯´æ˜: æƒé‡ä»¥ U8 æ‰“åŒ…å­˜å‚¨, æ¯ä¸ª uint8 = 4 ä¸ª 2-bit ä¸‰å€¼æƒé‡")
    print(f"  ç¼–ç : 2'b00=-1, 2'b01=0, 2'b10=+1, 2'b11=0(padding)")

    with safe_open(str(SAFETENSORS_PATH), framework="numpy") as f:
        for t in sample_tensors:
            name = t["name"]
            packed = f.get_tensor(name)
            unpacked = unpack_ternary_u8(packed)
            total = len(unpacked)

            n_pos = int((unpacked == 1).sum())
            n_neg = int((unpacked == -1).sum())
            n_zero = int((unpacked == 0).sum())
            unique_vals = set(unpacked.tolist()[:1000])

            # å®é™…é€»è¾‘ç»´åº¦ (æ‰“åŒ…å‰)
            real_shape = (t["shape"][0] * 4, t["shape"][1]) if len(t["shape"]) == 2 else t["shape"]

            print(f"\n  {name}:")
            print(f"    æ‰“åŒ…å½¢çŠ¶: {t['shape']} (U8) -> è§£åŒ…å½¢çŠ¶: {real_shape} (ternary)")
            print(f"    è§£åŒ…åå”¯ä¸€å€¼: {sorted(unique_vals)}")
            is_ternary = unique_vals.issubset({-1, 0, 1})
            if is_ternary:
                print(f"    âœ… ä¸‰å€¼æƒé‡: +1={n_pos/total*100:.1f}%, 0={n_zero/total*100:.1f}%, -1={n_neg/total*100:.1f}%")
            else:
                print(f"    âŒ éä¸‰å€¼æƒé‡ (å…± {len(unique_vals)} ä¸ªå”¯ä¸€å€¼)")


if __name__ == "__main__":
    import io
    import contextlib

    # åŒæ—¶è¾“å‡ºåˆ°ç»ˆç«¯å’Œæ–‡ä»¶
    buf = io.StringIO()
    with contextlib.redirect_stdout(buf):
        main()

    output = buf.getvalue()
    # æ‰“å°åˆ°ç»ˆç«¯
    print(output, end="")
    # ä¿å­˜åˆ°åŒç›®å½•ä¸‹çš„ analysis_report.txt
    report_path = MODEL_DIR / "analysis_report.txt"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(output)
    print(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
